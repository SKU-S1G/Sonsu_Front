import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
from PIL import ImageFont, ImageDraw, Image
import time

# ëª¨ë¸ ë° ë°ì´í„° ì •ë³´
model_path = 'last.tflite'  # TFLite ëª¨ë¸ ê²½ë¡œ
actions = [
    'ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ì‚¬ë‘í•©ë‹ˆë‹¤', 'ì–´ë¨¸ë‹ˆ', 'ì•„ë²„ì§€', 'ë™ìƒ', 'ì˜', 'ëª»', 'ê°„ë‹¤', 'ë‚˜',
    'ì´ë¦„', 'ë§Œë‚˜ë‹¤', 'ë°˜ê°‘ë‹¤', 'ë¶€íƒ', 'í•™êµ', 'ìƒì¼', 'ì›”', 'ì¼', 'ë‚˜ì´', 'ë³µìŠµ', 'í•™ìŠµ', 'ëˆˆì¹˜', 'ì˜¤ë‹¤', 'ë§', 'ê³±ë‹¤',
    'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…',
    'ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£',
    'ã…', 'ã…’', 'ã…”', 'ã…–', 'ã…¢', 'ã…š', 'ã…Ÿ'
]  # í•™ìŠµí•œ ë™ì‘ ë¦¬ìŠ¤íŠ¸
seq_length = 30  # ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì‹œí€€ìŠ¤ ê¸¸ì´

# ëª¨ë“ˆ ë‚´ë¶€ ìƒíƒœ ë³€ìˆ˜
_current_question = None
_game_result = None
_question_time = 0  # ë¬¸ì œê°€ ì œì‹œëœ ì‹œê°„
_min_confidence = 0.8  # ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
_warm_up_time = 3  # ë¬¸ì œ ì œì‹œ í›„ íŒë³„ ì‹œì‘ê¹Œì§€ì˜ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
_confidence_score = None

# ìƒíƒœ ì ‘ê·¼ í•¨ìˆ˜
def set_game_state(question, result):
    global _current_question, _game_result, _question_time
    _current_question = question
    _game_result = result
    _question_time = time.time()  # ë¬¸ì œê°€ ìƒˆë¡œ ì„¤ì •ë˜ì—ˆì„ ë•Œ ì‹œê°„ ê¸°ë¡

def get_game_state():
    return _current_question, _game_result

def get_confidence_score():  # ğŸ”§ ì¶”ê°€: ì •í™•ë„ ë°˜í™˜ í•¨ìˆ˜
    return _confidence_score

# TFLite ëª¨ë¸ ë¡œë“œ
interpreter = tf.lite.Interpreter(model_path=model_path)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# MediaPipe ëª¨ë¸ ë¡œë“œ
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils
holistic = mp_holistic.Holistic(
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ì¹´ë©”ë¼ ë° ì‹œí€€ìŠ¤ ì €ì¥ ë¦¬ìŠ¤íŠ¸
cap = None
seq = []
is_recognizing = False

def draw_text(img, text, position, font, color=(0, 255, 0)):
    """PILì„ ì´ìš©í•´ í•œê¸€ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜"""
    img_pil = Image.fromarray(img)
    draw = ImageDraw.Draw(img_pil)
    draw.text(position, text, font=font, fill=color)
    return np.array(img_pil)

def generate_frames(target_width=480, target_height=640):  # í•´ìƒë„ ì¸ì ì¶”ê°€
    global cap, seq, is_recognizing, _current_question, _game_result, _question_time

    if cap is None or not cap.isOpened():
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # ì›ë³¸ ì…ë ¥ í•´ìƒë„
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    seq = []
    is_recognizing = True
    last_prediction_time = 0
    prediction_cooldown = 1.0

    while is_recognizing:
        ret, img = cap.read()
        if not ret:
            break

        img = cv2.flip(img, 1)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        result = holistic.process(img_rgb)

        joint_list = []
        if result.left_hand_landmarks:
            for lm in result.left_hand_landmarks.landmark:
                joint_list.append([lm.x, lm.y, lm.z])
        else:
            joint_list.extend([[0, 0, 0]] * 21)

        if result.right_hand_landmarks:
            for lm in result.right_hand_landmarks.landmark:
                joint_list.append([lm.x, lm.y, lm.z])
        else:
            joint_list.extend([[0, 0, 0]] * 21)

        if result.pose_landmarks:
            for lm in result.pose_landmarks.landmark:
                joint_list.append([lm.x, lm.y, lm.z])
        else:
            joint_list.extend([[0, 0, 0]] * 33)

        current_time = time.time()
        elapsed_since_question = current_time - _question_time
        ready_to_predict = elapsed_since_question >= _warm_up_time

        if not ready_to_predict and _current_question:
            countdown = max(0, int(_warm_up_time - elapsed_since_question))
            img = draw_text(img, f"ì¤€ë¹„í•˜ì„¸ìš”... {countdown}ì´ˆ", (10, 50), (0, 0, 255))

        if joint_list:
            joint_list = np.array(joint_list).flatten()
            seq.append(joint_list)

            if len(seq) > seq_length:
                seq.pop(0)

            if ready_to_predict and len(seq) == seq_length and (current_time - last_prediction_time >= prediction_cooldown):
                input_data = np.expand_dims(np.array(seq), axis=0).astype(np.float32)
                interpreter.set_tensor(input_details[0]['index'], input_data)
                interpreter.invoke()
                prediction = interpreter.get_tensor(output_details[0]['index'])[0]

                predicted_action = actions[np.argmax(prediction)]
                confidence = np.max(prediction)

                if confidence >= _min_confidence:

                    if _current_question:
                        if predicted_action == _current_question:
                            _game_result = "ì •ë‹µì…ë‹ˆë‹¤!"
                        else:
                            _game_result = "í‹€ë ¸ìŠµë‹ˆë‹¤!"
                    else:
                        _game_result = "ë¬¸ì œê°€ ì¶œì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                    last_prediction_time = current_time

        if result.left_hand_landmarks:
            mp_drawing.draw_landmarks(img, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        if result.right_hand_landmarks:
            mp_drawing.draw_landmarks(img, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        if result.pose_landmarks:
            mp_drawing.draw_landmarks(img, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS)

        # ë¦¬ì‚¬ì´ì¦ˆ ì‹œ ì „ë‹¬ë°›ì€ í•´ìƒë„ë¡œ ì¶œë ¥ìš© ì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆ
        img = cv2.resize(img, (target_width, target_height))

        _, buffer = cv2.imencode('.jpg', img)
        frame = buffer.tobytes()
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')

    if cap is not None and cap.isOpened():
        cap.release()